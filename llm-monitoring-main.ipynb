{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4668db48-ebef-4529-ad64-f40435e82153",
   "metadata": {},
   "source": [
    "# Large language model monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563b5106-61db-4852-9778-bc7109051c38",
   "metadata": {},
   "source": [
    "Maintaining the performance of machine learning models in production is essential. Model monitoring tracks key metrics like accuracy, latency, and resource usage, to identify issues such as data drift and model decay. LLMs, like any other models, need to be monitored! \n",
    "\n",
    "This demo creates a banking chatbot. At first the bot answers any question on any subject. You'll monitor, fine-tune, and redploy it, to restrict it to answering only banking-related questions. To do so, you'll build an automated feedback loop from detecting accuracy drift, retraining, and redeployment.\n",
    "\n",
    "This notebook guides you through setting up an effective model monitoring system that leverages LLMs (LLM as a Judge) to maintain high standards for deployed models. It demonstrates how to prepare and evaluate a good prompt for the LLM judge, deploy model monitoring applications, assess the performance of a pre-trained model, fine-tune it using the ORPO technique on the supplied dataset, show the monitoring results for the fine-tuned model and finally, set an automatic pipeline to automatically fine-tune the model once the monitor raised an alert.\n",
    "\n",
    "### Note: in order to run this notebook you have to provide PostgreSQL\n",
    "\n",
    "\n",
    "\n",
    "![](./images/feedback_loop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145bbe5a-cd74-4386-aafe-80f93a0e5339",
   "metadata": {},
   "source": [
    "## In this notebook\n",
    "\n",
    "1. [Create an MLRun project](#create-an-mlrun-project)\n",
    "2. [LLM as a Judge](#llm-as-a-judge)\n",
    "3. [Model Monitoring](#mlrun-model-monitoring)\n",
    "4. [ORPO Fine-tuning](#orpo-fine-tuning)\n",
    "5. [Automated Feedback Loop](#automated-feedback-loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e613b1f",
   "metadata": {},
   "source": [
    "## Create an MLRun project\n",
    "\n",
    "MLRun project is a container for all your work on a particular ML or gen AI application. Projects host functions, workflows, artifacts, features, and configuration (parameters, secrets, source, etc.). The MLRun project is created by running the function [`mlrun.get_or_create_project`](https://docs.mlrun.org/en/stable/api/mlrun.projects.html#mlrun.projects.get_or_create_project). This creates the project and customizes it according to the [project_setup.py](./project_setup.py). \n",
    "\n",
    "After creating the project, you enable model monitoring."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import mlrun\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "from src.llm_as_a_judge import OpenAIJudge"
   ],
   "id": "2186eac3109c41eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e99f06d",
   "metadata": {},
   "source": [
    "# Create the project:\n",
    "project = mlrun.get_or_create_project(\n",
    "    name=\"llm-monitoring\",\n",
    "    parameters={\"image\":\".llm-serving\",\n",
    "        \"node_selector\": None, # Change to a node selector that is used in GPUs nodes\n",
    "    },\n",
    "    user_project = True,\n",
    "    context=\"./src\",\n",
    ")\n",
    "\n",
    "secrets = mlrun.set_env_from_file('env.env', return_dict=True)\n",
    "project.set_secrets(secrets)"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Enable model monitoring\n",
    "from src.model_monitoring_utils import enable_model_monitoring\n",
    "\n",
    "# If this project was running with MM enabled pre-1.8.0, disable the old model monitoring to update configurations\n",
    "project.disable_model_monitoring(delete_stream_function=True)\n",
    "\n",
    "enable_model_monitoring(project=project, base_period=2)"
   ],
   "id": "78a9e4fe68462400"
  },
  {
   "cell_type": "markdown",
   "id": "1221853f-f743-4748-af10-033698ee6c27",
   "metadata": {},
   "source": [
    "## LLM as a judge "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5efdd45-1edb-463f-bc7f-1f82b8dd4134",
   "metadata": {},
   "source": [
    "Using LLMs as judges for model monitoring is an innovative approach that leverages their remarkable language understanding capabilities. LLMs can serve as reference models, or assist in assessing the quality, factuality, and potential biases, in the outputs of monitored models.\n",
    "\n",
    "This demo makes two attempts to prompt engineer ChatGPT to be the judge, illustrating how the prompt templates affect the efficacy of the LLM as a judge.  \n",
    "\n",
    "First, get an evaluation set and an accuracy measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776fa9e3-3b67-44f5-845a-228e106ebe5e",
   "metadata": {},
   "source": [
    "### Load the banking dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6011439f-c945-4836-87be-5310fea63e59",
   "metadata": {},
   "source": [
    "Use a small dataset to teach the model to answer only banking-related questions. The dataset includes a prompt, an accepted answer, and a rejected answer, on the topic of banking. The dataset contains guardrails that prompt, in addition to the banking related prompts, to teach the model not to answer non-related questions. \n",
    "\n",
    "This dataset is also used later to train the model using ORPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5524deb-f4c1-4bf2-8a28-d58f0f382402",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"mlrun/banking-orpo-new\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = dataset.shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9098f434-10cd-4bc4-87d2-e497b3223eea",
   "metadata": {},
   "source": [
    "Preview of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987fd074-b3ce-46ae-9220-94893b51acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de21e5e7-6f19-4353-becf-781437ec2462",
   "metadata": {},
   "source": [
    "### Create an accuracy metric\n",
    "\n",
    "This simple function acts as the judge's accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681b870-bc99-42d2-9fa5-5b970a4489e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(col1, col2):\n",
    "    # Calculate the number of matching values\n",
    "    matching_values = sum(col1 == col2)\n",
    "\n",
    "    # Calculate the total number of values\n",
    "    total_values = len(col1)\n",
    "\n",
    "    # Calculate the percentage of matching values\n",
    "    return matching_values / total_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842034c-0693-4a23-a53a-c651801c68f9",
   "metadata": {},
   "source": [
    "### Create the evaluation set\n",
    "\n",
    "To prepare the dataset for evaluation, take 10% of the data and split it into two:\n",
    "* The first portion contains questions and answers as expected, meaning that the answers are taken from the **chosen** column.\n",
    "* The second portion contains questions with unexpected answers, meaning that the answers are taken from the **rejected** column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb8f723-8cc1-43a5-aa27-22e75ea5179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only 10% of the data:\n",
    "orpo_dataset = dataset.to_pandas().sample(frac=0.1, random_state=42, ignore_index=True)\n",
    "middle_index = len(orpo_dataset) // 2\n",
    "\n",
    "# Make 50% of the data correct and 50% of the data incorrect:\n",
    "chosen = (\n",
    "    orpo_dataset.iloc[:middle_index]\n",
    "    .rename(columns={\"prompt\": \"question\", \"chosen\": \"answer\"})\n",
    "    .drop(\"rejected\", axis=1)\n",
    ")\n",
    "rejected = (\n",
    "    orpo_dataset.iloc[middle_index:]\n",
    "    .rename(columns={\"prompt\": \"question\", \"rejected\": \"answer\"})\n",
    "    .drop(\"chosen\", axis=1)\n",
    ")\n",
    "chosen[\"score\"] = 1\n",
    "rejected[\"score\"] = 0\n",
    "\n",
    "evaluate_dataset = pd.concat([chosen, rejected])\n",
    "labels = evaluate_dataset[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61bf3e-e4d7-4cf9-9835-4ea3935d7045",
   "metadata": {},
   "source": [
    "And here is the evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d2f3d4-3d7d-4ceb-9392-1ee79b4108d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7a5c0-ae5e-411d-bbee-1e7637afd77f",
   "metadata": {},
   "source": [
    "### Prompt engineering the judge - **first attempt**\n",
    "\n",
    "For the first attempt, use a naive basic prompt for the judge, passing a custom string representing the template with placeholders for `question`, `answer` and `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf26355-9c69-4bda-88ce-8849af0925c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bad_banking_template = \"\"\"\n",
    "1 score if the model answers for banking questions, 0 score otherwise\n",
    "The question:\n",
    "{question}\n",
    "The answer:\n",
    "{answer}\n",
    "Answer the following question and return as a python dictionary:\n",
    "{{\"score\": <a numerical score for the response>,\n",
    "\"explanation\": <a string value of an explanation about how did you determine the score>}}\n",
    "[Output]:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfdba00-8cb9-4ee4-8595-ae7e46fe656b",
   "metadata": {},
   "source": [
    "Use a judging class that uses OpenAI GPT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad13cf-8348-47e8-b8f5-1f810e494cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = mlrun.get_secret_or_env(\"OPENAI_MODEL\")\n",
    "# Creating the OpenAI Judge\n",
    "judge = OpenAIJudge(\n",
    "    judge_type=\"custom-grading\",\n",
    "    metric_name=\"Restrict-to-banking\",\n",
    "    model_name=OPENAI_MODEL,\n",
    "    prompt_template=bad_banking_template,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06744e24-0d71-422c-b5e0-a9a3b3db8db6",
   "metadata": {},
   "source": [
    "Call the judge, and then the accuracy function to get the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0d1f56-f084-469f-b428-258477ec32a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_attempt_result = judge.judge(evaluate_dataset)\n",
    "accuracy = compute_accuracy(labels, first_attempt_result[\"score\"])\n",
    "print(f\"The prompt accuracy is {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb9756c-f350-499b-b641-8b5592065e93",
   "metadata": {},
   "source": [
    "As you can see, these results are **not good**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a7a081-39e9-49eb-8d79-ae8a486b4131",
   "metadata": {},
   "source": [
    "### Prompt engineering the judge - **second attempt**\n",
    "\n",
    "This run uses a different prompt template: `single-grading`. This template is much more detailed (not like the bad_banking_temlate used previously). It uses `prompt_config` with the following fields:\n",
    "- `name`: the name of the metric.\n",
    "- `definition`: A short definition of the metric.\n",
    "- `rubric`: The rubric explanation for each score.\n",
    "- `examples`: This section is very important. It gives examples for each rubric and cases that are covered. They include one example of a banking-related question and one example of non-banking-related question. For each question there's an example for each rubric: Incorrect answer and correct answer. This helps the judge to make the right decision.\n",
    "\n",
    "The judge takes the single-grading template and fills in the values from the provided configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83faa2af-5ed4-46f5-94d8-ae00e2c4ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "restrict_to_banking_config = {\n",
    "    \"name\": \"Restrict to banking\",\n",
    "    \"definition\": \"The Restrict to banking is a metric used to evaluate if a model responds exclusively to questions pertaining to banking topics.\",\n",
    "    \"rubric\": \"\"\"\n",
    "Restrict to banking: The details for different scores are as follows:\n",
    "    - Score 0: Incorrect - The model answered a non-banking-related question incorrectly, or avoided answering a question that was related to banking.\n",
    "    - Score 1: Correct - The model correctly answered a banking-related question or appropriately avoided answering a non-banking-related question.\n",
    "\"\"\",\n",
    "    \"examples\": \"\"\"\n",
    "Question: What is the process to apply for a mortgage?\n",
    "    Score 0: Incorrect\n",
    "    Answer: \"As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?\"\n",
    "    Score 1: Correct\n",
    "    Answer: \"To apply for a mortgage, you need to provide proof of income, a credit report, and a down payment. You can start by contacting a bank or a mortgage lender.\"\n",
    "Question: What is the best recipe for chocolate cake?\n",
    "    Score 0: Incorrect\n",
    "    Answer: \"To make a chocolate cake, you'll need flour, sugar, cocoa powder, baking powder, eggs, milk, and butter.\"\n",
    "    Score 1: Correct\n",
    "    Answer: \"As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?\"\n",
    "\"\"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189072b-6deb-4c16-8a35-cfeb05391013",
   "metadata": {},
   "source": [
    "Now run the same process as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672c5fc6-3aad-4173-97a1-221c6a3552d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = OpenAIJudge(\n",
    "    judge_type=\"single-grading\",\n",
    "    metric_name=\"Restrict-to-banking\",\n",
    "    model_name=OPENAI_MODEL,\n",
    "    prompt_config=restrict_to_banking_config,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924a47c1-9e48-406f-8a2b-3351177ed731",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_attempt_result = judge.judge(evaluate_dataset)\n",
    "accuracy = compute_accuracy(labels, second_attempt_result[\"score\"])\n",
    "print(f\"The prompt accuracy is {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d8a9e5-37aa-492b-a331-92f829dedebb",
   "metadata": {},
   "source": [
    "Now that the **LLM works well as a judge**, the next stage is the actual model monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a25133-4be7-43be-bc33-9dd68674382a",
   "metadata": {},
   "source": [
    "<a id=\"mlrun-model-monitoring\"></a>\n",
    "## Model monitoring\n",
    "\n",
    "MLRun's model monitoring includes built-in model monitoring and reporting capabilities. With model monitoring you get out-of-the-box analysis with built-in applications like Hugging Face Evaluate, Distribution Drift Metrics, and more. For more information, see the [MLRun documentation](https://docs.mlrun.org/en/stable/concepts/model-monitoring.html).\n",
    "\n",
    "This demo uses the custom judge application `OpenAIJudge` that was just built."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7c06e-64db-4655-bd77-c7dbf0215531",
   "metadata": {},
   "source": [
    "### Deploy the model monitoring application\n",
    "\n",
    "First, deploy the model monitoring application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define application requirements\n",
    "requirements = ['openai==1.108.0',\n",
    "'deepeval==2.5.5',\n",
    "'llama-index==0.14.2',\n",
    "'llama-index-core==0.14.2',\n",
    "'langchain==0.2.17',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9528eb",
   "metadata": {},
   "source": [
    "### \"LLM as a judge\" model monitoring function\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e31576a009f2ff93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae55b44-8f13-40ca-a50a-662d9cd3fb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "application = project.set_model_monitoring_function(\n",
    "    func=\"src/llm_as_a_judge.py\",\n",
    "    application_class=\"LLMAsAJudgeApplication\",\n",
    "    name=\"llm-as-a-judge\",\n",
    "    image='mlrun/mlrun',\n",
    "    requirements=requirements,\n",
    "    framework=\"openai\",\n",
    "    judge_type=\"single-grading\",\n",
    "    metric_name=\"restrict_to_banking\",\n",
    "    model_name=OPENAI_MODEL,\n",
    "    prompt_config=restrict_to_banking_config,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf1af0-f4cd-48c3-a9fb-8a1369748d76",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "application_deployment = project.deploy_function(application)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be7492a-c64e-4532-8ccd-ff09b7557554",
   "metadata": {},
   "source": [
    "### DeepEval model monitoring function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1345d-f58b-4e8b-9329-21cf428b6f9f",
   "metadata": {},
   "source": [
    "Use DeepEval as a judge to measure performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf13a1c-304d-445a-b8a6-46ec7f003e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepeval_application = project.set_model_monitoring_function(\n",
    "    func=\"src/deepeval_as_a_judge.py\",\n",
    "    application_class=\"DeepEvalAsAJudgeApplication\",\n",
    "    name=\"deepeval-as-a-judge\",\n",
    "    image=application_deployment.function.status.container_image,\n",
    "    metric_name=\"restrict_to_banking_deepeval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6a4fa2-5dc1-48be-992a-fe85386d21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deepeval_application_deployment = project.deploy_function(deepeval_application)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6530bd07-8a0f-44d8-ac10-8ea39de626ac",
   "metadata": {},
   "source": [
    "### Deploy the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd171097-960e-4971-8b2e-d2c371823fbd",
   "metadata": {},
   "source": [
    "First log the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd94444-b83e-4547-80a3-294688102af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlrun.features import Feature\n",
    "\n",
    "# Log the model to the project:\n",
    "base_model = \"google-gemma-2b\"\n",
    "project.log_model(\n",
    "    base_model,\n",
    "    model_file=\"src/model-iris.pkl\",\n",
    "    inputs=[Feature(value_type=\"str\", name=\"question\")],\n",
    "    outputs=[Feature(value_type=\"str\", name=\"answer\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e314e-290a-4adc-ad7a-608325cca4ca",
   "metadata": {},
   "source": [
    "Now, create a model server to serve this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a670975-28fe-4839-ba33-d3693a88f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the serving function to evaluate the base model:\n",
    "serving_function = project.get_function(\"llm-server\")\n",
    "\n",
    "# Add the logged model:\n",
    "serving_function.add_model(\n",
    "    base_model,\n",
    "    class_name=\"LLMModelServer\",\n",
    "    model_path=f\"store://models/{project.name}/{base_model}:latest\",\n",
    "    model_name=\"google/gemma-2b\",\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"max_length\": 80,\n",
    "    },\n",
    "    device_map=\"cuda:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e7beb-a2b5-4cfc-bb26-58b97ea703a0",
   "metadata": {},
   "source": [
    "To enable monitoring, use the method `set_tracking`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002712f4-1058-474e-9e86-1ce2b37a1abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_function.set_tracking()\n",
    "serving_function.spec.readiness_timeout = 1200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d83eae-a904-4161-bcc9-f25ed09befb4",
   "metadata": {},
   "source": [
    "And lastly, deploy it as a serverless function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43f694-762a-409c-b053-358672cb99fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = serving_function.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d6db53-6514-4af6-b6c8-8eecc5043f48",
   "metadata": {},
   "source": [
    "### Configure an alert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b25bf-028d-40b3-aa7b-275ad190ac80",
   "metadata": {},
   "source": [
    "Define an alert to be triggered on degradation of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c4369-16c7-42b4-9057-6e623be63a09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "app_name = \"llm-as-a-judge\"\n",
    "result_name = \"restrict-to-banking\"\n",
    "message = \"Model perf detected\"\n",
    "alert_config_name = \"restrict-to-banking\"\n",
    "dummy_url = \"dummy-webhook.default-tenant.app.llm-dev.iguazio-cd1.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ee93a4-b296-42a6-9f2d-d9ed549670c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get Endpoint ID:\n",
    "endpoints = mlrun.get_run_db().list_model_endpoints(project=project.name)\n",
    "ep_id = endpoints.endpoints[0].metadata.uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144ddc2-5552-4670-ba15-c21b19b4164f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mlrun.model_monitoring.helpers import get_result_instance_fqn\n",
    "prj_alert_obj = get_result_instance_fqn(\n",
    "    ep_id, app_name=app_name, result_name=result_name\n",
    ")\n",
    "\n",
    "webhook_notification = mlrun.common.schemas.Notification(\n",
    "    name=\"webhook\",\n",
    "    kind=\"webhook\",\n",
    "    params={\"url\": dummy_url},\n",
    "    when=[\"completed\", \"error\"],\n",
    "    severity=\"debug\",\n",
    "    message=\"Model perf detected\",\n",
    "    condition=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea519ff5-0d4c-4f39-bd00-57c77b54fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlrun.common.schemas.alert as alert_constants\n",
    "import mlrun.common.schemas.alert as alert_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecfcf75-d01f-49c7-92da-32b22c87f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_config = mlrun.alerts.alert.AlertConfig(\n",
    "    project=project.name,\n",
    "    name=alert_config_name,\n",
    "    summary=alert_config_name,\n",
    "    severity=alert_constants.AlertSeverity.HIGH,\n",
    "    entities=alert_constants.EventEntities(\n",
    "        kind=alert_constants.EventEntityKind.MODEL_ENDPOINT_RESULT,\n",
    "        project=project.name,\n",
    "        ids=[prj_alert_obj],\n",
    "    ),\n",
    "    trigger=alert_constants.AlertTrigger(\n",
    "        events=[alert_objects.EventKind.MODEL_PERFORMANCE_DETECTED, alert_objects.EventKind.MODEL_PERFORMANCE_SUSPECTED]\n",
    "    ),\n",
    "    criteria=alert_constants.AlertCriteria(count=1, period=\"10m\"),\n",
    "    notifications=[\n",
    "        alert_constants.AlertNotification(notification=webhook_notification)\n",
    "    ],\n",
    "    reset_policy=mlrun.common.schemas.alert.ResetPolicy.MANUAL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d85fb-f146-4923-9372-49a890dd25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.store_alert_config(alert_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11348e6-e53a-4e5e-a680-7c18f4298316",
   "metadata": {},
   "source": [
    "### Check the performance of the base model\n",
    "\n",
    "To evaluate the base model, ask it a number of questions and give it some requests. \n",
    "\n",
    "**It's expected to fail**, since it was not trained to restrict its answer only to question related to banking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de9d2b4-b000-4caf-99fb-3eea578069d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_questions = [\n",
    "    \"What is a mortgage?\",\n",
    "    \"How does a credit card work?\",\n",
    "    \"Who painted the Mona Lisa?\",\n",
    "    \"Please plan me a 4-days trip to north Italy\",\n",
    "    \"Write me a song\",\n",
    "    \"How much people are there in the world?\",\n",
    "    \"What is climate change?\",\n",
    "    \"How does the stock market work?\",\n",
    "    \"Who wrote 'To Kill a Mockingbird'?\",\n",
    "    \"Please plan me a 3-day trip to Paris\",\n",
    "    \"Write me a poem about the ocean\",\n",
    "    \"How many continents are there in the world?\",\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"How does a hybrid car work?\",\n",
    "    \"Who invented the telephone?\",\n",
    "    \"Please plan me a week-long trip to New Zealand\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9657e239-a049-480f-8936-752ab09e7327",
   "metadata": {},
   "source": [
    "The monitoring application is periodic, and is activated in a set time-period. Therefore, you need to create a questioning function that is timed such that the questions span several time-periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2b6ba-d864-41d3-b0e2-4dbb81562a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_model(questions, serving_function, base_model):\n",
    "    for question in questions:\n",
    "        seconds = 0.5\n",
    "        # Invoking the pretrained model:\n",
    "        ret = serving_function.invoke(\n",
    "            path=f\"/v2/models/{base_model}/infer\",\n",
    "            body={\"inputs\": [question]},\n",
    "        )\n",
    "        time.sleep(seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaeb589-4460-4273-b030-86430cfd9735",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(20):\n",
    "    question_model(\n",
    "        questions=example_questions,\n",
    "        serving_function=serving_function,\n",
    "        base_model=base_model,\n",
    "    )\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb33a0-971c-42f9-b5ec-afb1212ad1ba",
   "metadata": {},
   "source": [
    "The Grafana model monitoring page shows the base model's scores. After 10 minutes of traffic it looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef8d216-a456-4c2f-b3e5-b92964d0599a",
   "metadata": {},
   "source": [
    "![](./images/grafana_before.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f8310-4efb-4ade-a54a-646b5af9b690",
   "metadata": {},
   "source": [
    "As you can see, the base model is not the best at answering only banking-related questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80851fb2-9911-4976-8cd4-298c7a6b6938",
   "metadata": {},
   "source": [
    "### Evaluate the model using DeepEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793d59d-8272-4771-8cd6-4ff9564bf3f4",
   "metadata": {},
   "source": [
    "Let's also see how to use DeepEval to measure the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f9ca1c-f9cd-4439-8705-d5096d15c8f2",
   "metadata": {},
   "source": [
    "#### Banking question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb382d9-9a54-4407-888c-48e9174536d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import (\n",
    "    AnswerRelevancyMetric,\n",
    "    HallucinationMetric,\n",
    ")\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = mlrun.get_secret_or_env(\"OPENAI_API_KEY\")\n",
    "os.environ[\"OPENAI_BASE_URL\"]= mlrun.get_secret_or_env(\"OPENAI_API_BASE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5914b5-2ce4-4a06-aff6-1d71116ef206",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the process to apply for a mortgage?\"\n",
    "ret = serving_function.invoke(\n",
    "    path=f\"/v2/models/{base_model}/infer\",\n",
    "    body={\"inputs\": [question]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f1a112-705c-479a-9eb5-3a11642a1425",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret['outputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8daddbb-0f04-42f4-a7fb-83c8b30ab486",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case1 = LLMTestCase(\n",
    "    input=question,\n",
    "    actual_output=ret['outputs'][0],\n",
    "    expected_output=\"To apply for a mortgage, you need to provide proof of income, a credit report, and a down payment. You can start by contacting a bank or a mortgage lender.\",\n",
    "    retrieval_context=[\"For mortgage application you need to provide proof of income, a credit report, and a down payment\"]\n",
    ")\n",
    "\n",
    "answer_relevancy_metric1 = AnswerRelevancyMetric(threshold=0.5)\n",
    "\n",
    "results1 = evaluate(test_cases=[test_case1], metrics=[answer_relevancy_metric1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab9479-5205-4e73-9c0d-336c602b9fab",
   "metadata": {},
   "source": [
    "#### Non-banking question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069fc882-b8bf-4a69-8225-6a8f1c524895",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who painted the Mona Lisa?\"\n",
    "ret = serving_function.invoke(\n",
    "    path=f\"/v2/models/{base_model}/infer\",\n",
    "    body={\"inputs\": [question]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18b491-1576-498e-b40d-f71e3035dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret['outputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1265674-2af4-4a1f-8452-d850d88df6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case2 = LLMTestCase(\n",
    "    input=question,\n",
    "    actual_output=ret['outputs'][0],\n",
    "    expected_output=\"As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?\",\n",
    "    context=[\"This is a banking agent that allowed to talk on banking related issues only.\"]\n",
    ")\n",
    "\n",
    "answer_relevancy_metric2 = HallucinationMetric(threshold=0.5,model=OPENAI_MODEL)\n",
    "\n",
    "results2 = evaluate(test_cases=[test_case2], metrics=[answer_relevancy_metric2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a92380a-727f-4c58-a5a0-9346700ead38",
   "metadata": {},
   "source": [
    "<a id=\"orpo-fine-tuning\"></a>\n",
    "## ORPO fine-tuning\n",
    "\n",
    "To fine-tune the model, take the requests sent to the model (both questions related to, and not related to, banking), build a dataset according to the [ORPO](https://huggingface.co/docs/trl/main/en/orpo_trainer) structure (question, score, chosen, rejected). Then, re-train the model with it.\n",
    "\n",
    "The result in a fine-tuned model that only answers banking-questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c16d6dc-bd0e-4db0-9d5e-2a7a9b966359",
   "metadata": {},
   "source": [
    "### Build the training set\n",
    "\n",
    "First, fetch the data collected by the model monitoring from the initial traffic to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e9e40-ad6a-4b8e-8dd5-88d43f2c7d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = project.list_artifacts(kind=\"dataset\")\n",
    "ds_key = datasets[0][\"spec\"][\"db_key\"]\n",
    "input_ds = f\"store://datasets/{project.name}/{ds_key}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fb2e83",
   "metadata": {},
   "source": [
    "Now, use OpenAI ChatGPT to generate expected outputs (you can see the function [here](./src/generate_ds.py))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e05f11cec8df6",
   "metadata": {},
   "source": [
    "> **Note:** To upload the generated dataset you need to provide a Hugging Face repo that your account token has permission to upload datasets to, for example: `hf_repo_id:mlrun/banking-orpo-new`, if None skip the dataset upload to hf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae978eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.build_function(\"generate-ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355bf89c-9729-4c9a-8e66-0088ff33d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = project.run_function(\n",
    "    function=\"generate-ds\",\n",
    "    handler=\"generate_ds\",\n",
    "    params={\"input_ds\": input_ds},\n",
    "    outputs=[\"new-train-ds\", \"dataset\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994cb3cc-92aa-4ce1-9c0b-0dfe5a8d136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0767dea-5cd2-4a9d-ac08-a418357b916b",
   "metadata": {},
   "source": [
    "Now we have a new dataset for the model tuning stored in Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c064094-0739-4180-93a3-4873f186f995",
   "metadata": {},
   "source": [
    "### Fine-tune the model\n",
    "\n",
    "It's time to fine-tune the model using the ORPO algorithm, so that the model only answers the banking-related questions.\n",
    "\n",
    "[ORPO](https://arxiv.org/abs/2403.07691) is a new method designed to simplify and improve the process of fine-tuning language models to align with user preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556c120-85b1-4ff5-9395-949ea4a50644",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build the train function image \n",
    "train_func = project.build_function(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105b3a2-77bd-450e-a01c-10e2a424862f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "project.run_function(\n",
    "    function=\"train\",\n",
    "    params={\n",
    "        \"dataset\": \"mlrun/banking-orpo-opt\",\n",
    "        \"base_model\": \"google/gemma-2b\",\n",
    "        \"new_model\": \"mlrun/gemma-2b-bank-v0.2\",\n",
    "        \"device\": \"cuda:0\",\n",
    "    },\n",
    "    handler=\"train\",\n",
    "    outputs=[\"model\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15032f46-9dfc-4d88-87c7-610d26189f9e",
   "metadata": {},
   "source": [
    "### Check the performance of the fine-tuned model\n",
    "\n",
    "Load and deploy the trained model to see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac65b0-76ff-419f-910a-6fb856b212f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_function.add_model(\n",
    "    base_model,\n",
    "    class_name=\"LLMModelServer\",\n",
    "    llm_type=\"HuggingFace\",\n",
    "    model_name=\"google/gemma-2b\",\n",
    "    adapter=\"mlrun/gemma-2b-bank-v0.2\",\n",
    "    model_path=f\"store://models/{project.name}/{base_model}:latest\",\n",
    "    generate_kwargs={\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"num_return_sequences\": 1,\n",
    "        \"max_length\": 80,\n",
    "    },\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "serving_function.set_tracking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3364b62-8897-4037-832b-51a27490fe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = serving_function.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3cf2a5-d489-4747-9c39-0645be0362d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for i in range(20):\n",
    "    question_model(\n",
    "        questions=example_questions,\n",
    "        serving_function=serving_function,\n",
    "        base_model=base_model,\n",
    "    )\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8815b08b-d9de-4499-994f-9139b104954f",
   "metadata": {},
   "source": [
    "The Grafana model monitoring page shows a high pass rate and a high guardrails score:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a549dc-d19a-4895-a837-89c52cd3791f",
   "metadata": {},
   "source": [
    "![](./images/grafana_after.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7721119e-ce10-47c7-a62f-99bc2c334ea0",
   "metadata": {},
   "source": [
    "### Evaluate the model using DeepEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f2e7d-2d43-4548-bcda-6fd987c6dc35",
   "metadata": {},
   "source": [
    "Again, test the fine tuned model's performance using DeepEval:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452db891-cefd-4489-8423-048b6e3a2f82",
   "metadata": {},
   "source": [
    "#### Banking-related question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bd473e-e713-4745-9a81-8a41a453f041",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is a bank mortgage?\"\n",
    "ret = serving_function.invoke(\n",
    "    path=f\"/v2/models/{base_model}/infer\",\n",
    "    body={\"inputs\": [question]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ff6c40-b9e4-4c1c-a230-1f466c1b3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret['outputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668677ff-f6ad-4a97-8b86-0af7b6d773a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case1 = LLMTestCase(\n",
    "    input=question,\n",
    "    actual_output=ret['outputs'][0],\n",
    "    expected_output=\"A mortgage is a loan used to purchase a house or other real estate.\",\n",
    "    retrieval_context=[\"A mortgage is a banking related term\"]\n",
    ")\n",
    "\n",
    "answer_relevancy_metric1 = AnswerRelevancyMetric(threshold=0.5)\n",
    "\n",
    "results1 = evaluate(test_cases=[test_case1], metrics=[answer_relevancy_metric1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe8c26-8aff-427c-b238-de12d7238646",
   "metadata": {},
   "source": [
    "#### Banking question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce08612-ebec-482d-a043-d17144ed2b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who painted the Mona Lisa?\"\n",
    "ret = serving_function.invoke(\n",
    "    path=f\"/v2/models/{base_model}/infer\",\n",
    "    body={\"inputs\": [question]},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12313d-754d-4608-9680-da6e0bbcb761",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ret['outputs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f38e7-44b2-40fc-8b94-c157fc0e3719",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case2 = LLMTestCase(\n",
    "    input=question,\n",
    "    actual_output=ret['outputs'][0],\n",
    "    expected_output=\"As a banking agent, I am not allowed to talk on this subject. Is there anything else I can help with?\",\n",
    "    context=[\"This is a banking agent that allowed to talk on banking related issues only.\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ff582-efe0-4247-89f0-18c9d86ee3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_relevancy_metric2 = HallucinationMetric(threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3298fd-9c61-48fa-a85c-e170528fc742",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = evaluate(test_cases=[test_case2], metrics=[answer_relevancy_metric2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
